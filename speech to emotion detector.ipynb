{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1zRAPVF-_8_ipQUxbpjIMoydKLYJ3_VSz","authorship_tag":"ABX9TyMeem46BxBg62bHTEdeTH2h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"9_il_Zfljx_e","executionInfo":{"status":"error","timestamp":1743048750546,"user_tz":-330,"elapsed":1831,"user":{"displayName":"Brijen Shinde","userId":"10463722676204842628"}},"outputId":"6e478f8d-26eb-4f88-9c93-9e402fcc9f2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Found 0 audio files.\n"]},{"output_type":"error","ename":"ValueError","evalue":"‚ùå No valid audio features extracted!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f2b6be9bd29b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Training model and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0mpredict_emotions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-f2b6be9bd29b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     model = MLPClassifier(\n","\u001b[0;32m<ipython-input-1-f2b6be9bd29b>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(data_path, test_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå No valid audio features extracted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: ‚ùå No valid audio features extracted!"]}],"source":["import os\n","import glob\n","import numpy as np\n","import librosa\n","import soundfile\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.neural_network import MLPClassifier\n","\n","# Defining all emotions\n","int2emotion = {\n","    \"01\": \"neutral\",\n","    \"02\": \"calm\",\n","    \"03\": \"happy\",\n","    \"04\": \"sad\",\n","    \"05\": \"angry\",\n","    \"06\": \"fearful\",\n","    \"07\": \"disgust\",\n","    \"08\": \"surprised\"\n","}\n","\n","# Include all emotions in the classification\n","AVAILABLE_EMOTIONS = set(int2emotion.values())  # Includes all 8 emotions\n","\n","# Feature extraction function\n","def extract_feature(file_name, mfcc=True, chroma=True, mel=True):\n","    try:\n","        with soundfile.SoundFile(file_name) as sound_file:\n","            X = sound_file.read(dtype=\"float32\")\n","            sample_rate = sound_file.samplerate\n","            stft = np.abs(librosa.stft(X)) if chroma else None\n","\n","            result = np.array([])\n","            if mfcc:\n","                mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n","                result = np.hstack((result, mfccs))\n","            if chroma:\n","                chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n","                result = np.hstack((result, chroma))\n","            if mel:\n","                mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n","                result = np.hstack((result, mel))\n","        return result\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Error processing {file_name}: {e}\")\n","        return None\n","\n","#  Loading dataset and training model\n","def load_dataset(data_path, test_size=0.2):\n","    X, y = [], []\n","\n","    files = glob.glob(os.path.join(data_path, \"*.wav\"))\n","    print(f\"üîç Found {len(files)} audio files.\")\n","\n","    for file in files:\n","        basename = os.path.basename(file)\n","        parts = basename.split(\"-\")\n","\n","        if len(parts) < 3:\n","            print(f\"‚ö†Ô∏è Skipping {basename} (Invalid filename format)\")\n","            continue\n","\n","        emotion_id = parts[2]  # Extract emotion ID\n","        emotion = int2emotion.get(emotion_id)\n","\n","        if emotion is None:\n","            print(f\"‚ö†Ô∏è Skipping {basename} (Unknown emotion ID: {emotion_id})\")\n","            continue\n","\n","        features = extract_feature(file)\n","        if features is not None:\n","            X.append(features)\n","            y.append(emotion)\n","\n","    if not X:\n","        raise ValueError(\"‚ùå No valid audio features extracted!\")\n","\n","    X, y = np.array(X), np.array(y)\n","    encoder = LabelEncoder()\n","    y = encoder.fit_transform(y)\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    return train_test_split(X, y, test_size=test_size, random_state=42), encoder, scaler\n","\n","# Training the model\n","def train_model(data_path=\"/content/Test\"):\n","    (X_train, X_test, y_train, y_test), encoder, scaler = load_dataset(data_path)\n","\n","    model = MLPClassifier(\n","        alpha=0.01, batch_size=256, epsilon=1e-08,\n","        hidden_layer_sizes=(300,), learning_rate=\"adaptive\", max_iter=500\n","    )\n","\n","    print(\"üöÄ Training model...\")\n","    model.fit(X_train, y_train)\n","\n","    accuracy = model.score(X_test, y_test) * 100\n","    print(f\"‚úÖ Model trained with accuracy: {accuracy:.2f}%\")\n","\n","    # Save model & encoder\n","    joblib.dump(model, \"emotion_model.pkl\")\n","    joblib.dump(encoder, \"encoder.pkl\")\n","    joblib.dump(scaler, \"scaler.pkl\")\n","    return model, encoder, scaler\n","\n","# Predicting emotions from new audio files\n","def predict_emotions(audio_folder=\"/Test\"):\n","    # Load trained model\n","    model = joblib.load(\"emotion_model.pkl\")\n","    encoder = joblib.load(\"encoder.pkl\")\n","    scaler = joblib.load(\"scaler.pkl\")\n","\n","    files = glob.glob(os.path.join(audio_folder, \"*.wav\"))\n","    print(f\"üîé Detecting emotions in {len(files)} files...\")\n","\n","    for file in files:\n","        features = extract_feature(file)\n","        if features is None:\n","            print(f\"‚ö†Ô∏è Skipping {os.path.basename(file)} (Feature extraction failed)\")\n","            continue\n","\n","        features = scaler.transform([features])  # Normalize input\n","        emotion_index = model.predict(features)[0]  # Get prediction\n","        emotion = encoder.inverse_transform([emotion_index])[0]\n","\n","        print(f\"üéµ {os.path.basename(file)} ‚Üí Detected Emotion: {emotion}\")\n","\n","# Training model and predict\n","train_model(\"/content/Test\")\n","predict_emotions(\"/content/Test\")\n"]}]}